# .gitignore

```
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions
.env
.env.local
.DS_Store

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

```

# .vscode/launch.json

```json
{
    "version": "0.2.0",
    "configurations": [
      {
        "name": "Next.js: debug server-side",
        "type": "node-terminal",
        "request": "launch",
        "command": "npm run dev"
      },
      {
        "name": "Next.js: debug client-side",
        "type": "chrome",
        "request": "launch",
        "url": "http://localhost:3000"
      },
      {
        "name": "Next.js: debug full stack",
        "type": "node",
        "request": "launch",
        "program": "${workspaceFolder}/node_modules/.bin/next",
        "runtimeArgs": ["--inspect"],
        "skipFiles": ["<node_internals>/**"],
        "serverReadyAction": {
          "action": "debugWithEdge",
          "killOnServerStop": true,
          "pattern": "- Local:.+(https?://.+)",
          "uriFormat": "%s",
          "webRoot": "${workspaceFolder}"
        }
      }
    ]
  }
```

# app/favicon.ico

This is a binary file of the type: Binary

# app/globals.css

```css
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --background: #ffffff;
  --foreground: #171717;
}

@media (prefers-color-scheme: dark) {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
  }
}

body {
  color: var(--foreground);
  background: var(--background);
  font-family: Arial, Helvetica, sans-serif;
}

```

# app/layout.tsx

```tsx
import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        {children}
      </body>
    </html>
  );
}

```

# app/page.tsx

```tsx
import { getServerSession } from 'next-auth/next';
import LoginButton from '@/components/LoginButton';

export default function Home() {
  return (
    <div className="min-h-screen bg-gray-100 flex flex-col items-center justify-center">
      <div className="bg-white p-8 rounded-lg shadow-md">
        <h1 className="text-3xl font-bold mb-6">Welcome to Audio Rants</h1>
        <p className="mb-6">Share your thoughts through voice recordings</p>
        <a
          href="/record"
          className="bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600"
        >
          Sign In
        </a>
      </div>
    </div>
  );
}
```

# app/record/page.js

```js
import VideoRecorder from '@/components/VideoRecorder';

export default function RecordPage() {
  return (
    <div className="min-h-screen bg-gray-100 p-8">
      <h1 className="text-3xl font-bold mb-8 text-center">Record Your Video</h1>
      <VideoRecorder />
    </div>
  );
}
```

# app/view/page.js

```js

```

# components/AudioRecorder.js

```js
'use client';
import { useReactMediaRecorder } from 'react-media-recorder';
import { useState } from 'react';

export default function AudioRecorder() {
  const [isUploading, setIsUploading] = useState(false);
  const { status, startRecording, stopRecording, mediaBlobUrl } = useReactMediaRecorder({
    audio: true,
    video: false,
  });

  const handleUpload = async () => {
    if (!mediaBlobUrl) return;

    setIsUploading(true);
    try {
      const blob = await fetch(mediaBlobUrl).then(r => r.blob());
      const formData = new FormData();
      formData.append('file', blob, 'recording.wav');

      const response = await fetch('/api/upload', {
        method: 'POST',
        body: formData,
      });

      if (!response.ok) throw new Error('Upload failed');

      // Trigger transcription
      const { fileKey } = await response.json();
      await fetch('/api/transcribe', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ fileKey }),
      });
    } catch (error) {
      console.error('Error:', error);
    } finally {
      setIsUploading(false);
    }
  };

  return (
    <div className="p-4 bg-white rounded-lg shadow">
      <div className="mb-4">
        <p>Status: {status}</p>
      </div>
      <div className="flex gap-2">
        <button
          onClick={startRecording}
          disabled={status === 'recording'}
          className="bg-red-500 text-white px-4 py-2 rounded disabled:bg-gray-400"
        >
          Start Recording
        </button>
        <button
          onClick={stopRecording}
          disabled={status !== 'recording'}
          className="bg-gray-500 text-white px-4 py-2 rounded disabled:bg-gray-400"
        >
          Stop Recording
        </button>
        <button
          onClick={handleUpload}
          disabled={!mediaBlobUrl || isUploading}
          className="bg-blue-500 text-white px-4 py-2 rounded disabled:bg-gray-400"
        >
          {isUploading ? 'Uploading...' : 'Upload'}
        </button>
      </div>
      {mediaBlobUrl && (
        <div className="mt-4">
          <audio src={mediaBlobUrl} controls />
        </div>
      )}
    </div>
  );
}
```

# components/LiveCaptions.tsx

```tsx
import React, { useState, useEffect, useRef } from 'react';
import * as speechsdk from 'microsoft-cognitiveservices-speech-sdk';

interface LiveCaptionsProps {
  audioStream: MediaStream | null;
  isRecording: boolean;
  setTranscription: (transcription: string) => void;
}

const LiveCaptions: React.FC<LiveCaptionsProps> = ({ audioStream, isRecording, setTranscription }) => {
  const [tempTranscript, setTempTranscript] = useState<string>('');
  const recognizerRef = useRef<speechsdk.SpeechRecognizer | null>(null);

  useEffect(() => {
    if (!audioStream || !isRecording) {
      if (recognizerRef.current) {
        recognizerRef.current.stopContinuousRecognitionAsync();
        recognizerRef.current = null; // Clear the ref
      }
      return;
    }

    const speechConfig = speechsdk.SpeechConfig.fromSubscription(
      process.env.NEXT_PUBLIC_AZURE_SPEECH_KEY ?? '',
      process.env.NEXT_PUBLIC_AZURE_SPEECH_REGION ?? ''
    );
    speechConfig.speechRecognitionLanguage = 'en-US';

    const audioConfig = speechsdk.AudioConfig.fromStreamInput(audioStream);
    const recognizer = new speechsdk.SpeechRecognizer(speechConfig, audioConfig);

    recognizer.recognizing = (_: speechsdk.Recognizer, e: speechsdk.SpeechRecognitionEventArgs) => {
      setTempTranscript(e.result.text);
    };

    recognizer.recognized = (_: speechsdk.Recognizer, e: speechsdk.SpeechRecognitionEventArgs) => {
      if (e.result.reason === speechsdk.ResultReason.RecognizedSpeech) {
        setTranscription(prev => `${prev} ${e.result.text}`);
        setTempTranscript(''); // Clear temporary transcript
      }
    };

    recognizer.sessionStopped = () => {
      console.log("Speech recognition session stopped");
      recognizer.stopContinuousRecognitionAsync(); // Ensure it stops
    };

    recognizer.canceled = (sender: speechsdk.Recognizer, cancellationEventArgs: speechsdk.SpeechRecognitionCanceledEventArgs) => {
      console.log("Speech recognition canceled: " + cancellationEventArgs.reason);
      if (cancellationEventArgs.reason === speechsdk.CancellationReason.Error) {
        console.error("Error details: " + cancellationEventArgs.errorDetails);
      }
      recognizer.stopContinuousRecognitionAsync(); // Ensure it stops
    };

    recognizer.startContinuousRecognitionAsync();
    recognizerRef.current = recognizer;

    return () => {
      if (recognizerRef.current) {
        recognizerRef.current.stopContinuousRecognitionAsync();
      }
    };
  }, [audioStream, isRecording, setTranscription]);

  return (
    <div className="absolute bottom-16 left-0 right-0 p-4">
      <div className="bg-black bg-opacity-50 text-white p-2 rounded text-center">
        {tempTranscript || 'Listening...'}
      </div>
    </div>
  );
};

export default LiveCaptions;

```

# components/LoginButton.js

```js
'use client';
import { signIn } from 'next-auth/react';

export default function LoginButton() {
  return (
    <button
      onClick={() => signIn('google')}
      className="bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600"
    >
      Sign in with Google
    </button>
  );
}
```

# components/Sidebar.js

```js
'use client';
import { signOut } from 'next-auth/react';
import Link from 'next/link';

export default function Sidebar() {
  return (
    <div className="w-64 bg-gray-800 min-h-screen p-4 text-white">
      <nav className="flex flex-col h-full">
        <div className="flex-grow">
          <Link 
            href="/record" 
            className="block py-2 px-4 hover:bg-gray-700 rounded mb-2"
          >
            Record
          </Link>
          <Link 
            href="/history" 
            className="block py-2 px-4 hover:bg-gray-700 rounded"
          >
            History
          </Link>
        </div>
        <button
          onClick={() => signOut()}
          className="w-full py-2 px-4 bg-red-500 hover:bg-red-600 rounded"
        >
          Logout
        </button>
      </nav>
    </div>
  );
}
```

# components/TranscriptionView.js

```js

```

# components/VideoRecorder.tsx

```tsx
'use client'
import { useState, useRef, useEffect } from 'react';
import { Camera, Mic, Maximize2, MinimizeIcon } from 'lucide-react';
import LiveCaptions from './LiveCaptions'; // Import LiveCaptions component

export default function VideoRecorder() {
  const [isRecording, setIsRecording] = useState(false);
  const [videoUrl, setVideoUrl] = useState(null);
  const [isCameraOn, setIsCameraOn] = useState(true);
  const [isMicOn, setIsMicOn] = useState(true);
  const [isFullscreen, setIsFullscreen] = useState(false);
  const [transcription, setTranscription] = useState('');
  const [recordingTime, setRecordingTime] = useState(0);
  const [devices, setDevices] = useState<{ videoDevices: MediaDeviceInfo[], audioDevices: MediaDeviceInfo[] }>({
    videoDevices: [],
    audioDevices: []
  });
  const [selectedCamera, setSelectedCamera] = useState('');
  const [selectedMic, setSelectedMic] = useState('');

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const videoRef = useRef<HTMLVideoElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const chunksRef = useRef([]);
  const containerRef = useRef(null);
  const timerRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    initializeCamera();
    getAvailableDevices();
    return () => {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
    };
  }, []);

  useEffect(() => {
    if (isRecording) {
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);
    } else {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
      setRecordingTime(0);
    }
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    };
  }, [isRecording]);

  const getAvailableDevices = async () => {
    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const videoDevices = devices.filter(device => device.kind === 'videoinput');
      const audioDevices = devices.filter(device => device.kind === 'audioinput');
      
      setDevices({ videoDevices, audioDevices });
      
      if (videoDevices.length) setSelectedCamera(videoDevices[0].deviceId);
      if (audioDevices.length) setSelectedMic(audioDevices[0].deviceId);
    } catch (err) {
      console.error('Error getting devices:', err);
    }
  };

  const initializeCamera = async () => {
    try {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }

      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: selectedCamera ? { deviceId: selectedCamera } : true,
        audio: selectedMic ? { deviceId: selectedMic } : true
      });
      
      streamRef.current = stream;
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
      }
    } catch (err) {
      console.error('Error accessing camera:', err);
    }
  };

  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  const startRecording = async () => {
    try {
      await initializeCamera(); // Reinitialize with both audio and video

      if (streamRef.current) {
        mediaRecorderRef.current = new MediaRecorder(streamRef.current);
      }
      mediaRecorderRef.current = mediaRecorderRef.current;
      chunksRef.current = [];
      setTranscription('');

      mediaRecorderRef.current.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };

      mediaRecorderRef.current.onstop = () => {
        const blob = new Blob(chunksRef.current, { type: 'video/webm' });
        const url = URL.createObjectURL(blob);
        setVideoUrl(url);
      };

      mediaRecorderRef.current.start();
      setIsRecording(true);
    } catch (err) {
      console.error('Error starting recording:', err);
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      streamRef.current?.getTracks().forEach((track: MediaStreamTrack) => track.stop());
      initializeCamera(); // Restart preview
    }
  };

  const handleDeviceChange = async (deviceId: string, type: string) => {
    if (type === 'video') {
      setSelectedCamera(deviceId);
    } else {
      setSelectedMic(deviceId);
    }

    if (!isRecording) {
      await initializeCamera();
    }
  };

  const toggleCamera = () => {
    setIsCameraOn(prev => !prev);
    if (streamRef.current) {
      streamRef.current.getVideoTracks().forEach(track => {
        track.enabled = !isCameraOn;
      });
    }
  };

  const toggleMic = () => {
    setIsMicOn(prev => !prev);
    if (streamRef.current) {
      streamRef.current.getAudioTracks().forEach(track => {
        track.enabled = !isMicOn;
      });
    }
  };

  const toggleFullscreen = () => {
    if (!document.fullscreenElement) {
      containerRef.current?.requestFullscreen();
      setIsFullscreen(true);
    } else {
      document.exitFullscreen();
      setIsFullscreen(false);
    }
  };

  return (
    <div ref={containerRef} className="p-6 bg-white rounded-lg shadow-lg max-w-2xl mx-auto">
      <div className="mb-4 flex gap-4">
        <select 
          value={selectedCamera}
          onChange={(e) => handleDeviceChange(e.target.value, 'video')}
          className="p-2 border rounded"
        >
          {devices.videoDevices.map(device => (
            <option key={device.deviceId} value={device.deviceId}>
              {device.label || `Camera ${device.deviceId.slice(0, 5)}`}
            </option>
          ))}
        </select>
        <select 
          value={selectedMic}
          onChange={(e) => handleDeviceChange(e.target.value, 'audio')}
          className="p-2 border rounded"
        >
          {devices.audioDevices.map(device => (
            <option key={device.deviceId} value={device.deviceId}>
              {device.label || `Microphone ${device.deviceId.slice(0, 5)}`}
            </option>
          ))}
        </select>
      </div>

      <div className="aspect-video bg-gray-100 rounded-lg overflow-hidden mb-4 relative">
        <video
          ref={videoRef}
          autoPlay
          playsInline
          muted
          className="w-full h-full object-cover"
        />
        {videoUrl && !isRecording && (
          <video
            src={videoUrl}
            controls
            className="w-full h-full object-cover"
          />
        )}
        
        {isRecording && (
          <div className="absolute top-4 left-4 bg-red-500 text-white px-3 py-1 rounded-full flex items-center gap-2">
            <span className="w-2 h-2 bg-white rounded-full animate-pulse" />
            {formatTime(recordingTime)}
          </div>
        )}

        {isRecording && streamRef.current && (
          <LiveCaptions 
            audioStream={streamRef.current} 
            isRecording={isRecording}
            setTranscription={setTranscription}
          />
        )}

        <div className="absolute bottom-4 right-4 flex gap-2">
          <button
            onClick={toggleCamera}
            className={`p-2 rounded-full ${isCameraOn ? 'bg-blue-500' : 'bg-gray-500'}`}
          >
            <Camera className="w-5 h-5 text-white" />
          </button>
          <button
            onClick={toggleMic}
            className={`p-2 rounded-full ${isMicOn ? 'bg-blue-500' : 'bg-gray-500'}`}
          >
            <Mic className="w-5 h-5 text-white" />
          </button>
          <button
            onClick={toggleFullscreen}
            className="p-2 rounded-full bg-gray-500"
          >
            {isFullscreen ? (
              <MinimizeIcon className="w-5 h-5 text-white" />
            ) : (
              <Maximize2 className="w-5 h-5 text-white" />
            )}
          </button>
        </div>
      </div>

      <div className="flex gap-4 justify-center mb-4">
        {!isRecording ? (
          <button
            onClick={startRecording}
            className="bg-red-500 text-white px-6 py-2 rounded-lg hover:bg-red-600 flex items-center gap-2"
          >
            <span className="w-3 h-3 rounded-full bg-white" />
            Record
          </button>
        ) : (
          <button
            onClick={stopRecording}
            className="bg-gray-500 text-white px-6 py-2 rounded-lg hover:bg-gray-600"
          >
            Stop
          </button>
        )}
        {videoUrl && !isRecording && (
          <button
            className="bg-blue-500 text-white px-6 py-2 rounded-lg hover:bg-blue-600"
          >
            Upload & Transcribe
          </button>
        )}
      </div>

      {transcription && (
        <div className="mt-4 p-4 bg-gray-50 rounded-lg">
          <h3 className="font-semibold mb-2">Transcription:</h3>
          <p>{transcription}</p>
        </div>
      )}
    </div>
  );
}

```

# eslint.config.mjs

```mjs
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
];

export default eslintConfig;

```

# lib/auth.js

```js

```

# lib/s3.js

```js
import { S3Client, PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3';

const s3Client = new S3Client({
  region: process.env.AWS_REGION,
  credentials: {
    accessKeyId: process.env.AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
  },
});

export async function uploadToS3(file, key) {
  const command = new PutObjectCommand({
    Bucket: process.env.AWS_S3_BUCKET,
    Key: key,
    Body: file,
  });

  return s3Client.send(command);
}

export async function getFromS3(key) {
  const command = new GetObjectCommand({
    Bucket: process.env.AWS_S3_BUCKET,
    Key: key,
  });

  return s3Client.send(command);
}
```

# lib/transcribe.js

```js

```

# next-env.d.ts

```ts
/// <reference types="next" />
/// <reference types="next/image-types/global" />

// NOTE: This file should not be edited
// see https://nextjs.org/docs/app/api-reference/config/typescript for more information.

```

# next.config.ts

```ts
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;

```

# package.json

```json
{
  "name": "audio-rant-app",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev --turbopack",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "lucide-react": "^0.475.0",
    "microsoft-cognitiveservices-speech-sdk": "^1.42.0",
    "next": "15.1.7",
    "next-auth": "^4.24.11",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "react-media-recorder": "^1.6.6"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "15.1.7",
    "postcss": "^8",
    "tailwindcss": "^3.4.1",
    "typescript": "^5"
  }
}

```

# postcss.config.mjs

```mjs
/** @type {import('postcss-load-config').Config} */
const config = {
  plugins: {
    tailwindcss: {},
  },
};

export default config;

```

# public/file.svg

This is a file of the type: SVG Image

# public/globe.svg

This is a file of the type: SVG Image

# public/next.svg

This is a file of the type: SVG Image

# public/vercel.svg

This is a file of the type: SVG Image

# public/window.svg

This is a file of the type: SVG Image

# README.md

```md
This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

\`\`\`bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
\`\`\`

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.

```

# tailwind.config.ts

```ts
import type { Config } from "tailwindcss";

export default {
  content: [
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
      },
    },
  },
  plugins: [],
} satisfies Config;

```

# tsconfig.json

```json
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}

```

